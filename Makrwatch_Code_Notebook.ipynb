{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greetings, Makrwatch! I'm Wes Clark, the Insight Fellow who worked on the demographic prediction project!\n",
    "\n",
    "# Welcome to my code notebook for the Makrwatch project! I'm going to split this up into relevant sections so you can mix and match code as you see fit.\n",
    "\n",
    "## I've done my best to document anything, but if you have any questions, I'm reachable at wesleycclark@gmail.com and I'll also be available via the Makrwatch Slack (wes_clark)\n",
    "\n",
    "### My deliverable to you all was a predictive model, which you've been able to see at the website link I gave to Andres. I'm hoping to migrate that to a real website at some point for you all to continue to play with, but for today this is a code notebook that contains all the relevant scripts/codes/functions that I used to build the product.\n",
    "\n",
    "### The sections I'm going to have are:\n",
    "\n",
    "<ol>\n",
    "<li> Scraping the Data Piece-Wise and Tag Processing</li>\n",
    "<li> Importing and Utilizing the Model</li>\n",
    "<li> Adding to the Model in the Future</li>\n",
    "<li> Additional Scripts I Wrote that may be Useful </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Scraping the Data Piece-Wise and Tag Processing\n",
    "\n",
    "#### This first part will be basically scraping new channels and new videos for their tag information for these models. \n",
    "#### In order to utilize most of the YouTube API scripts, a Youtube *Data* key is needed for the api_key. I've left it blank for these, but you can fill in with yours. \n",
    "#### Once we have the tags, we'll process them and remove unimportant words (articles, prepositions) and move to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is just to test if the video ID that is passed is legitimate and not a bad link\n",
    "# I specifically used this on the Flask app that you all have seen just to make sure I don't get bad requests\n",
    "\n",
    "import requests\n",
    "\n",
    "def is_url_ok(vid):\n",
    "\n",
    "    \n",
    "    full_url = 'https://www.youtube.com/watch?v='+vid\n",
    "    return(200 == requests.head(url).status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This next cell contains the code for acquiring the tags from a single video!\n",
    "#### (Assuming it has passed the above truth test of being a legit video ID!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#As I mentioned before, you'll pass two things here: a video ID for a youtube video, and a youtube DATA api key.\n",
    "import json\n",
    "\n",
    "r = requests.get('https://www.googleapis.com/youtube/v3/videos?part=snippet%2CcontentDetails%2Cstatistics&id=' + video_id + '&key='+api_key)\n",
    "data_json = r.json()\n",
    "\n",
    "### This next loop extracts all tags and puts them into one giant string - this is easier for the next level of processing.\n",
    "\n",
    "tag_string = ''\n",
    "for i in data_json['items'][0]['snippet']['tags']:\n",
    "    temp = i + ' '\n",
    "    tag_string += temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We want a string for subsequent processing. We'll eventually turn it back into a list later to pass to the model, but I've written some pre-processing code that normally handles strings. \n",
    "\n",
    "### The next little code snippet is for taking a channel link, either given in the form 'youtube.com/user' or 'youtube.com/channel' and extracting the tags from the 50 most recent uploaded videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "if 'youtube.com/user/' in test_url:\n",
    "    tag_list = []\n",
    "    hits = re.findall(\"/user/(\\w+)\",test_url)\n",
    "    print(hits[0])\n",
    "    r = requests.get('https://www.googleapis.com/youtube/v3/channels?part=contentDetails&forUsername='+hits[0]+'&key='+api_key)\n",
    "    playlist_json = r.json()\n",
    "    \n",
    "    try:\n",
    "        playlist_id = playlist_json['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    except:\n",
    "        playlist_id = ''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    video_list = []\n",
    "    try:   \n",
    "        videos = requests.get('https://www.googleapis.com/youtube/v3/playlistItems?part=contentDetails&maxResults=50&playlistId='+playlist_id+'&key='+api_key)\n",
    "        videos_json = videos.json()\n",
    "        \n",
    "        for i in range(len(videos_json['items'])):\n",
    "            video_list.append(videos_json['items'][i]['contentDetails']['videoId'])\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    for j in video_list:\n",
    "       \n",
    "        tag_request = requests.get('https://www.googleapis.com/youtube/v3/videos?part=snippet&id='+j+'&key='+ api_key)\n",
    "        tag_json = tag_request.json()\n",
    "        try:\n",
    "            tag_list.append(tag_json['items'][0]['snippet']['tags'])\n",
    "        except:\n",
    "            tag_list.append('')\n",
    "    \n",
    "elif 'youtube.com/channel' in test_url:\n",
    "    hits = re.findall('/channel/(\\w+)',test_url)\n",
    "    r = requests.get('https://www.googleapis.com/youtube/v3/channels?part=contentDetails&id='+hits[0]+'&key='+api_key)\n",
    "    print(hits[0])\n",
    "    playlist_json = r.json()\n",
    "    try:\n",
    "        playlist_id = playlist_json['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    except:\n",
    "        playlist_id = ''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    video_list = []\n",
    "    try:   \n",
    "        videos = requests.get('https://www.googleapis.com/youtube/v3/playlistItems?part=contentDetails&maxResults=50&playlistId='+playlist_id+'&key='+api_key)\n",
    "        videos_json = videos.json()\n",
    "        \n",
    "        for i in range(len(videos_json['items'])):\n",
    "            video_list.append(videos_json['items'][i]['contentDetails']['videoId'])\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    for j in video_list:\n",
    "       \n",
    "        tag_request = requests.get('https://www.googleapis.com/youtube/v3/videos?part=snippet&id='+j+'&key='+ api_key)\n",
    "        tag_json = tag_request.json()\n",
    "        try:\n",
    "            tag_list.append(tag_json['items'][0]['snippet']['tags'])\n",
    "        except:\n",
    "            tag_list.append('')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now that we have our list of tags from the channel, we'll turn it into a string for processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_string = ''\n",
    "for i in tag_list:\n",
    "    temp = ''\n",
    "    for j in range(len(i)):\n",
    "        temp += i[j] + ' '\n",
    "    tag_string += temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have our code in a giant string, we'll process it through two pre-defined functions to remove punctuation and unnecessary words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "punctuation = '!\"#?$%“”&\\'()’*+,—./:;<=>@[\\\\]^_`{|}~…' \n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "### This cell contains the necessary imports for getting rid of English stop words ('the', 'to', 'a'... and so on) \n",
    "### As well as punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_string = processText(removeStopwords(tag_string))\n",
    "string_list = []\n",
    "string_list.append(tag_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And just like that, we have the tags from either a single video or from a channel of videos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Importing and Utilizing the Model\n",
    "\n",
    "#### So my project was basically to build a model for you all that predicts age, gender, and country from the tags. \n",
    "\n",
    "#### The first thing we'll do is load the models. They're in pickle files, which are basically snapshots of code. \n",
    "#### Because of this, you can freely load them without having to re-train the models every time (which would be very time-consuming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#These next three models are associated with generating age/gender profiles.\n",
    "\n",
    "with open('KNN_Reg_Tags_AgeGender.p', 'rb') as pickle_file:\n",
    "    knn_age_gender = pickle.load(pickle_file)\n",
    "with open('SVD_BigTags.p', 'rb') as pickle_file:\n",
    "    svd = pickle.load(pickle_file)\n",
    "with open('TFIDF_BigTags.p', 'rb') as pickle_file:\n",
    "    tfidf = pickle.load(pickle_file)\n",
    "    \n",
    "    \n",
    "# These next two are associated with generating the location breakdown. The second file is just a quick country reference\n",
    "# based on the model. \n",
    "\n",
    "with open('knn_reg.p','rb') as pickle_file:\n",
    "    knn_country_model = pickle.load(pickle_file)\n",
    "        print('done')\n",
    "with open('country_dict.p','rb') as pickle_file:\n",
    "    country_dict = pickle.load(pickle_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So now we have imported our models. The next few lines of code are for taking the above tag list that we made and getting output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_desc = tfidf.transform(string_list)\n",
    "svd_transform = svd.transform(tag_desc)\n",
    "demo_data = knn_age_gender.predict(svd_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we have our data! Demo_data is a 14 member array, whose first 7 elements are Male 13-17, 18-24, 25-34, 35-44, 45-54, 55-64, and 65+, while the last 7 elements are the respective female age breakdowns. The numeric elements are the fractional breakdown (out of 1) , so if element 2 is .25, that means that 25% of the viewers are Male from 18-24.\n",
    "\n",
    "### Next we break down the country model, which picks up after the SVD transformation (basically breaking down the vocabulary into numbers, which can then be used as input for our predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy \n",
    "\n",
    "country_demo = knn_reg.predict(svd_transform)\n",
    "#Country demo is a large array containing an entry for every country that existed in the Youtube viewership. \n",
    "#As such, we only want the top 3, which the next line of code takes care of.\n",
    "\n",
    "arr = np.argsort(country_demo[0])[::-1][:3]\n",
    "\n",
    "# argsort returns the indices of the largest elements, and the [:3] returns the 3 largest. We're storing that in an array.\n",
    "# Now we just build up the output.\n",
    "\n",
    "\n",
    "country_demo_list = []\n",
    "country_demo_names = []\n",
    "total = 100\n",
    "\n",
    "def dict_replace(dictionary, key):  # this little quick function returns the country name from the argsort index\n",
    "    return(dictionary[key])\n",
    "\n",
    "\n",
    "for i in arr:\n",
    "    country_demo_list.append(country_demo[0][i]*100) #Giving a percentage \n",
    "    country_demo_names.append(dict_replace(country_dict,i)) #Giving us a two letter country code\n",
    "    total -= country_demo[0][i]*100 #Just appropriately decrementing our total\n",
    "\n",
    "country_demo_list.append(total)\n",
    "country_demo_names.append('The rest')\n",
    "\n",
    "\n",
    "#And now we have a list of country demographic percentages ('country_demo_list') and their names ('country_demo_names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So with that, we have predicted the age/gender breakdown and our country viewership numbers! That takes care of the prediction and modeling stage from our youtube video ID or youtube channel link! \n",
    "\n",
    "### In addition to this notebook, I'll also send over a zipped archive that contains all of the models (google drive?), as well as relevant information for the next chapter of adding to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.) Adding to the Model In the Future\n",
    "\n",
    "### So you all have a model, but there's always room to grow! Generally a model gains in potency when it has more test data to sample. While there were always be edge-cases that are difficult to predict, if in a year or so from now you're able to acquire more demographic information, it's always good to update!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In case you need to import all of the modules at once, I've aggregated most everything here\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import collections\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I've compiled a data table of all channels that Makrwatch had demographic information on for age/gender, which can be loaded as below. It's a pandas dataframe, so you can add new rows to it by concatenating to the end (pandas.concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('CombinedBigTagMergeDataFrame.p','rb') as pickle_file:\n",
    "    full_demo_tag_df = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Processed_Tags</th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>M6</th>\n",
       "      <th>M7</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22223</th>\n",
       "      <td>UCrHitTb3daDNagJ7NL8qOCw</td>\n",
       "      <td>curl my hair tutorial watch me subscribe sahm ...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>11.7</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>16.4</td>\n",
       "      <td>32.2</td>\n",
       "      <td>11.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22224</th>\n",
       "      <td>UCvVPZBHNGg4ZAYymVvzp6sA</td>\n",
       "      <td>acrylic nails acrylic nail designs unas de acr...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.5</td>\n",
       "      <td>34.7</td>\n",
       "      <td>27.3</td>\n",
       "      <td>10.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22225</th>\n",
       "      <td>UCmS-nTH3QH36VqXt8SfD8Qg</td>\n",
       "      <td>diy video kids crafts easy to spider halloween...</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.7</td>\n",
       "      <td>9.2</td>\n",
       "      <td>10.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>8.1</td>\n",
       "      <td>15.4</td>\n",
       "      <td>19.7</td>\n",
       "      <td>14.5</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22226</th>\n",
       "      <td>UCR_jPo2RgJLWP1Z-0s7Zccg</td>\n",
       "      <td>photoshop courses march madness layers smart o...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>18.9</td>\n",
       "      <td>26</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>8.7</td>\n",
       "      <td>9.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22227</th>\n",
       "      <td>UCdSjoa8r-bPbMJcvymeSrJQ</td>\n",
       "      <td>bonde dos bronze bronde lol jogadas de bronze ...</td>\n",
       "      <td>18.7</td>\n",
       "      <td>38.7</td>\n",
       "      <td>21.8</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Channel  \\\n",
       "22223  UCrHitTb3daDNagJ7NL8qOCw   \n",
       "22224  UCvVPZBHNGg4ZAYymVvzp6sA   \n",
       "22225  UCmS-nTH3QH36VqXt8SfD8Qg   \n",
       "22226  UCR_jPo2RgJLWP1Z-0s7Zccg   \n",
       "22227  UCdSjoa8r-bPbMJcvymeSrJQ   \n",
       "\n",
       "                                          Processed_Tags    M1    M2    M3  \\\n",
       "22223  curl my hair tutorial watch me subscribe sahm ...   0.5     4  11.7   \n",
       "22224  acrylic nails acrylic nail designs unas de acr...   0.6   2.7   3.5   \n",
       "22225  diy video kids crafts easy to spider halloween...   2.6   5.7   9.2   \n",
       "22226  photoshop courses march madness layers smart o...   2.5  18.9    26   \n",
       "22227  bonde dos bronze bronde lol jogadas de bronze ...  18.7  38.7  21.8   \n",
       "\n",
       "         M4   M5   M6   M7    F1    F2    F3    F4   F5   F6   F7  \n",
       "22223   6.4    3  1.5  0.7   2.2  16.4  32.2  11.8  5.6  2.1  1.8  \n",
       "22224   1.9    1  0.3  0.2  10.5  34.7  27.3  10.6  4.4  1.3  1.1  \n",
       "22225  10.4  3.7  0.9  0.9   8.1  15.4  19.7  14.5  5.4    2  1.5  \n",
       "22226  11.8  6.5  3.9  3.1   1.6   8.7   9.1   3.8  2.2  1.2  0.8  \n",
       "22227   7.4  2.3  0.7    2   0.7     2   2.3   2.3  0.9  0.1  0.1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_demo_tag_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And as you can see, it has the channel ID, a list of processed tags, as well as the respective age/gender groups for the YouTube demographics. \n",
    "\n",
    "### Similarly, there's a separate table for the country information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('country_full_info_table.p','rb') as pickle_file:\n",
    "    full_country_df = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GY</th>\n",
       "      <th>HK</th>\n",
       "      <th>YE</th>\n",
       "      <th>GP</th>\n",
       "      <th>AL</th>\n",
       "      <th>JP</th>\n",
       "      <th>MQ</th>\n",
       "      <th>ET</th>\n",
       "      <th>RW</th>\n",
       "      <th>SY</th>\n",
       "      <th>...</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>tag_list</th>\n",
       "      <th>Processed_Tags</th>\n",
       "      <th>Len_Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20364</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>32.7</td>\n",
       "      <td>33.2</td>\n",
       "      <td>13</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>hair makeup wig halloween cleopatra inspired m...</td>\n",
       "      <td>hair makeup wig halloween cleopatra inspired m...</td>\n",
       "      <td>23376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>12.1</td>\n",
       "      <td>23.9</td>\n",
       "      <td>13.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>plus size fashion plus size ootd plus size out...</td>\n",
       "      <td>plus size fashion plus size ootd plus size out...</td>\n",
       "      <td>57102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23814</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>25.5</td>\n",
       "      <td>25.8</td>\n",
       "      <td>7.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>indian youtuber indian skintone brown skintone...</td>\n",
       "      <td>indian youtuber indian skintone brown skintone...</td>\n",
       "      <td>22432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8748</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.6</td>\n",
       "      <td>28.2</td>\n",
       "      <td>16.4</td>\n",
       "      <td>13</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>amigos amigas vs mejores amigas bff best frien...</td>\n",
       "      <td>amigos amigas vs mejores amigas bff best frien...</td>\n",
       "      <td>3748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17555</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9</td>\n",
       "      <td>13.8</td>\n",
       "      <td>21.7</td>\n",
       "      <td>11.5</td>\n",
       "      <td>8.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>veganlovlie vegetable spaghetti spaghetti reci...</td>\n",
       "      <td>veganlovlie vegetable spaghetti spaghetti reci...</td>\n",
       "      <td>32112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        GY   HK   YE   GP   AL   JP   MQ   ET   RW   SY    ...       F1    F2  \\\n",
       "20364  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...      3.3  32.7   \n",
       "1413   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...      1.5  12.1   \n",
       "23814  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...      3.8  25.5   \n",
       "8748   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...     20.6  28.2   \n",
       "17555  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    ...      1.9  13.8   \n",
       "\n",
       "         F3    F4   F5   F6   F7  \\\n",
       "20364  33.2    13  5.8  1.8    1   \n",
       "1413   23.9  13.9  6.3  2.2  1.1   \n",
       "23814  25.8   7.7  3.3  1.5    1   \n",
       "8748   16.4    13  4.7  0.9  2.3   \n",
       "17555  21.7  11.5  8.2  5.7  3.2   \n",
       "\n",
       "                                                tag_list  \\\n",
       "20364  hair makeup wig halloween cleopatra inspired m...   \n",
       "1413   plus size fashion plus size ootd plus size out...   \n",
       "23814  indian youtuber indian skintone brown skintone...   \n",
       "8748   amigos amigas vs mejores amigas bff best frien...   \n",
       "17555  veganlovlie vegetable spaghetti spaghetti reci...   \n",
       "\n",
       "                                          Processed_Tags  Len_Tags  \n",
       "20364  hair makeup wig halloween cleopatra inspired m...     23376  \n",
       "1413   plus size fashion plus size ootd plus size out...     57102  \n",
       "23814  indian youtuber indian skintone brown skintone...     22432  \n",
       "8748   amigos amigas vs mejores amigas bff best frien...      3748  \n",
       "17555  veganlovlie vegetable spaghetti spaghetti reci...     32112  \n",
       "\n",
       "[5 rows x 164 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_country_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And this data frame has the 2 letter country codes with the respective breakdown per country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you ever need to retrain the model, then you can do the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=10, max_features=25000, strip_accents='unicode',\n",
    "                           analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,2), \n",
    "                           use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "\n",
    "#First we initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf.fit(full_demo_tag_df['Processed_Tags'])\n",
    "\n",
    "tfidf_descr = tfidf.fit_transform(full_demo_tag_df['Processed_Tags'])\n",
    "\n",
    "# Then we create a full TF-IDF model of all the tags in our corpus.\n",
    "#In this case, to feed it into the SVD model to build that too with the fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=50, random_state=0)\n",
    "svd.fit(tfidf_descr)\n",
    "svd_tfidf = svd.fit_transform(tfidf_descr)\n",
    "#Which we can feed into the model to fit the SVD model as well!\n",
    "#In order to build the subsequent regression models, we'll need the fit_transform again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd_test_tfidf_df =  pd.DataFrame(svd_tfidf)\n",
    "X = pd.DataFrame(full_demo_tag_df, columns = svd_test_tfidf_df.columns)\n",
    "y = pd.DataFrame(full_demo_tag_df, columns = ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7'])\n",
    "y = y.convert_objects(convert_numeric=True)\n",
    "\n",
    "# This is used to generate our features and labels for the regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_age_gender = KNeighborsRegressor(n_neighbors = 20, weights = 'uniform', p = 2)\n",
    "knn_age_gender.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The last few cells have all been similar - instantiate/initialize the model, and then fit our parameters to it. And these models are the ones that we loaded from the pickled file before. \n",
    "\n",
    "### Also, you can use the predict function on the knn_age_gender to get out our demo_data from section 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Additional Scripts I Wrote that may be Useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To generate our data frames for the initial age/gender, I used the following scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_2 = './source_2_demographics.json'\n",
    "\n",
    "channel_id_big = []\n",
    "age_range_big = []\n",
    "female_big = []\n",
    "male_big = []\n",
    "\n",
    "for line in open(file_2,'r'):\n",
    "    #print(line)\n",
    "    lineson = json.loads(line)\n",
    "    channel_id_big.append(lineson['channelId'])\n",
    "    age_range_big.append(lineson['range'])\n",
    "    try:    \n",
    "        male_big.append(lineson['male'])\n",
    "    except:\n",
    "        male_big.append('0')\n",
    "    try:\n",
    "        female_big.append(lineson['female'])\n",
    "    except:\n",
    "        female_big.append('0')\n",
    "        \n",
    "arrays = [channel_id_big,age_range_big]\n",
    "\n",
    "multi_index_big = pd.MultiIndex.from_arrays(arrays,names=['ID','Age_Range'])\n",
    "\n",
    "layer_df_big = pd.DataFrame({'Male':male_big,\n",
    "                        'Female': female_big},\n",
    "                       index = multi_index_big)\n",
    "\n",
    "demo_df = pd.DataFrame()\n",
    "\n",
    "demo_list = []\n",
    "\n",
    "\n",
    "for i in set(channel_id_big):\n",
    "    temp = layer_df_big.xs(i,level = 'ID', drop_level = False)\n",
    "    temp_list = []\n",
    "    temp_list.append(i)\n",
    "    for j in range(7):\n",
    "        try:\n",
    "            temp_list.append(temp.Male[j])\n",
    "        except:\n",
    "            temp_list.append(0)\n",
    "    for k in range(7):\n",
    "        try:\n",
    "            temp_list.append(temp.Female[k])\n",
    "        except:\n",
    "            temp_list.append(0)\n",
    "    attempt.append(temp_list)\n",
    "        \n",
    "demo_df = pd.DataFrame(columns = ['ID','M1','M2','M3','M4','M5','M6','M7','F1','F2','F3','F4','F5','F6','F7'])\n",
    "demo_df = demo_df.append(pd.DataFrame(demo_list, columns = demo_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>M6</th>\n",
       "      <th>M7</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCFtMeyaVYhicK5JTQ0mfNTA</td>\n",
       "      <td>1.6</td>\n",
       "      <td>11.8</td>\n",
       "      <td>23.6</td>\n",
       "      <td>12.3</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>13.4</td>\n",
       "      <td>14.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCeuG93VFLbag-hgCWv_a3uQ</td>\n",
       "      <td>5.7</td>\n",
       "      <td>27.4</td>\n",
       "      <td>44.6</td>\n",
       "      <td>9.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCcHsg3wQ8-1t4v7mhzM3sUg</td>\n",
       "      <td>1.6</td>\n",
       "      <td>12.8</td>\n",
       "      <td>30.1</td>\n",
       "      <td>19.2</td>\n",
       "      <td>10.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCpr25HXdBHjXxitvThcQgHQ</td>\n",
       "      <td>9.8</td>\n",
       "      <td>19.5</td>\n",
       "      <td>15.2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>8.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>8.5</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCzG6VQOETsDJYGDAai_9hwQ</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>9.9</td>\n",
       "      <td>36.7</td>\n",
       "      <td>27.5</td>\n",
       "      <td>10.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ID   M1    M2    M3    M4    M5   M6   M7   F1    F2  \\\n",
       "0  UCFtMeyaVYhicK5JTQ0mfNTA  1.6  11.8  23.6  12.3   5.4  2.0  1.2  2.7  13.4   \n",
       "1  UCeuG93VFLbag-hgCWv_a3uQ  5.7  27.4  44.6   9.1   3.8  0.4  0.9  0.9   3.3   \n",
       "2  UCcHsg3wQ8-1t4v7mhzM3sUg  1.6  12.8  30.1  19.2  10.2  6.0  4.2  0.8   4.6   \n",
       "3  UCpr25HXdBHjXxitvThcQgHQ  9.8  19.5  15.2   8.0   4.4  0.8  1.3  8.2  14.8   \n",
       "4  UCzG6VQOETsDJYGDAai_9hwQ  0.6   2.4   2.7   1.1   0.6  0.2  0.1  9.9  36.7   \n",
       "\n",
       "     F3    F4   F5   F6   F7  \n",
       "0  14.5   6.5  3.1  1.1  0.7  \n",
       "1   3.3   0.4  0.0  0.0  0.2  \n",
       "2   5.4   2.9  1.4  0.5  0.4  \n",
       "3   8.5   5.3  3.0  0.4  0.7  \n",
       "4  27.5  10.9  5.0  1.3  1.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And this is for generating a dataframe from the location information: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_2 = './source_2_audienceLocation.json'\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "big_json_df = pd.DataFrame()\n",
    "big_channel_list = []\n",
    "big_country_list = []\n",
    "big_percent_list = []\n",
    "with open(file_2,'r') as file:\n",
    "    for line in file:\n",
    "        dipdip = json.loads(line)\n",
    "        big_channel_list.append(dipdip['channelId'])\n",
    "        big_country_list.append(dipdip['country'])\n",
    "        big_percent_list.append(dipdip['percent'])\n",
    "        \n",
    "big_json_df['channelId'] = big_channel_list\n",
    "big_json_df['country'] = big_country_list\n",
    "big_json_df['percentage'] = big_percent_list\n",
    "\n",
    "big_df_channel_list = [i for i in  big_json_df['channelId']]\n",
    "big_country_set = set(big_country_list)\n",
    "\n",
    "big_df_channel_set = set(big_df_channel_list)\n",
    "big_zero_data = np.zeros(shape=(len(big_df_channel_set),len(big_country_set)))\n",
    "big_zero_country_df = pd.DataFrame(big_zero_data, index = big_df_channel_set, columns = big_country_set)\n",
    "\n",
    "# So now we've generated a dataframe from our country location json with all of the country codes and the channel IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KN</th>\n",
       "      <th>CZ</th>\n",
       "      <th>VG</th>\n",
       "      <th>CN</th>\n",
       "      <th>SY</th>\n",
       "      <th>TW</th>\n",
       "      <th>IQ</th>\n",
       "      <th>JP</th>\n",
       "      <th>JO</th>\n",
       "      <th>AM</th>\n",
       "      <th>...</th>\n",
       "      <th>CR</th>\n",
       "      <th>ES</th>\n",
       "      <th>SG</th>\n",
       "      <th>LU</th>\n",
       "      <th>OM</th>\n",
       "      <th>MG</th>\n",
       "      <th>PL</th>\n",
       "      <th>UY</th>\n",
       "      <th>CI</th>\n",
       "      <th>SI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UCFtMeyaVYhicK5JTQ0mfNTA</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UCIcbWJtXOjzm38IxPxaOQ_Q</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UC5VvHEF2hG9W4lIZBLINdrA</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UComrvL3jVbXiOTis8lPUYqQ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UCcHsg3wQ8-1t4v7mhzM3sUg</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           KN   CZ   VG   CN   SY   TW   IQ   JP   JO   AM  \\\n",
       "UCFtMeyaVYhicK5JTQ0mfNTA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "UCIcbWJtXOjzm38IxPxaOQ_Q  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "UC5VvHEF2hG9W4lIZBLINdrA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "UComrvL3jVbXiOTis8lPUYqQ  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "UCcHsg3wQ8-1t4v7mhzM3sUg  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "                         ...    CR   ES   SG   LU   OM   MG   PL   UY   CI  \\\n",
       "UCFtMeyaVYhicK5JTQ0mfNTA ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "UCIcbWJtXOjzm38IxPxaOQ_Q ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "UC5VvHEF2hG9W4lIZBLINdrA ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "UComrvL3jVbXiOTis8lPUYqQ ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "UCcHsg3wQ8-1t4v7mhzM3sUg ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "                           SI  \n",
       "UCFtMeyaVYhicK5JTQ0mfNTA  0.0  \n",
       "UCIcbWJtXOjzm38IxPxaOQ_Q  0.0  \n",
       "UC5VvHEF2hG9W4lIZBLINdrA  0.0  \n",
       "UComrvL3jVbXiOTis8lPUYqQ  0.0  \n",
       "UCcHsg3wQ8-1t4v7mhzM3sUg  0.0  \n",
       "\n",
       "[5 rows x 145 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_zero_country_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx,row in big_json_df.iterrows():\n",
    "    x = row['channelId']\n",
    "    y = row['country']\n",
    "    z = row['percentage']\n",
    "    big_zero_country_df.loc[x,y] = z\n",
    "    \n",
    "    \n",
    "# And this code is used to fill the dataframe with the respective percentages per country!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
